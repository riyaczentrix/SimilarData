import pandas as pd
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
import numpy as np
import sys
from collections import defaultdict

def format_simple_numbered_list(values):
    """
    Formats a list of values into a numbered string (e.g., "1. ValueA\n2. ValueB").
    Handles cases where 'values' might be a numpy array.
    Each item will be on a new line.
    """
    if len(values) == 0:
        return ""
    formatted_items = []
    for i, val in enumerate(values):
        # Ensure value is treated as string to avoid errors with non-string types.
        formatted_items.append(f"{i + 1}. {str(val).strip()}") # .strip() to clean whitespace
    return "\n".join(formatted_items)

def format_subject_output(unique_subjects):
    """
    Formats unique subjects into a 1.1, 1.2, ... string and creates a mapping.
    Returns the formatted string and a dictionary mapping original subject to its 1.X number.
    """
    subject_to_num_map = {}
    formatted_subjects = []
    # Sort subjects for consistent numbering
    sorted_subjects = sorted(unique_subjects)

    for i, subject_str in enumerate(sorted_subjects):
        num_str = f"1.{i + 1}"
        formatted_subjects.append(f"{num_str} {subject_str.strip()}")
        subject_to_num_map[subject_str] = num_str
    return "\n".join(formatted_subjects), subject_to_num_map

def format_sub_disposition_output(group_df, subject_to_num_map):
    """
    Formats sub_disposition_name with associated subject numbers (e.g., "(1.3, 1.4) Admin Requirement").
    """
    sub_disp_to_subject_nums = defaultdict(set) # Use set to avoid duplicate subject numbers

    for index, row in group_df.iterrows():
        sub_disp = str(row['sub_disposition_name']).strip()
        original_subject = str(row['subject']).strip()

        # Only process if sub_disp is not 'nan' or empty after stripping
        if sub_disp and sub_disp.lower() != 'nan':
            # Get the 1.X number for the subject of this row
            subject_num = subject_to_num_map.get(original_subject)
            if subject_num:
                sub_disp_to_subject_nums[sub_disp].add(subject_num)

    formatted_sub_dispositions = []
    # Sort sub-disposition names for consistent output order
    sorted_sub_disps = sorted(sub_disp_to_subject_nums.keys())

    for sub_disp in sorted_sub_disps:
        # Sort the subject numbers numerically (e.g., 1.1, 1.2, 1.10)
        # Convert to float for numeric sort, then back to string for display
        subject_nums_sorted = sorted(list(sub_disp_to_subject_nums[sub_disp]), key=lambda x: float(x.replace('1.', '')))
        subject_nums_str = ", ".join(subject_nums_sorted)
        formatted_sub_dispositions.append(f"({subject_nums_str}){sub_disp}")

    return "\n".join(formatted_sub_dispositions)


def consolidate_tickets(file_path, similarity_threshold=80):
    """
    Scans a CSV file, identifies rows with similar subjects, and consolidates them.

    Args:
        file_path (str): The path to the input CSV file.
        similarity_threshold (int): The minimum fuzzy matching score (0-100)
                                    to consider two subjects similar.

    Returns:
        pd.DataFrame: A new DataFrame with consolidated rows.
    """
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found. Please ensure the file exists.", file=sys.stderr)
        return pd.DataFrame()
    except Exception as e:
        print(f"Error reading CSV file: {e}", file=sys.stderr)
        return pd.DataFrame()

    # Ensure 'subject' column exists
    if 'subject' not in df.columns:
        print("Error: 'subject' column not found in the CSV file.", file=sys.stderr)
        return pd.DataFrame()

    # Initialize a list to store consolidated rows
    consolidated_rows = []
    # Keep track of indices already processed
    processed_indices = set()

    # Get a list of subjects and their original indices
    subjects_with_indices = [(str(sub), i) for i, sub in enumerate(df['subject'])]

    for i, (current_subject, current_idx) in enumerate(subjects_with_indices):
        if current_idx in processed_indices:
            continue # Skip if this row has already been processed as part of a group

        # Initialize a group for similar subjects
        current_group_indices = {current_idx}
        current_group_subjects = [current_subject] # Keep track of original subjects in group

        # Compare the current subject with all other unprocessed subjects
        for j, (other_subject, other_idx) in enumerate(subjects_with_indices):
            if other_idx in processed_indices or other_idx == current_idx:
                continue

            # Calculate similarity using token_sort_ratio for better robustness to word order
            score = fuzz.token_sort_ratio(current_subject, other_subject)

            if score >= similarity_threshold:
                current_group_indices.add(other_idx)
                current_group_subjects.append(other_subject)

        # Mark all found similar subjects as processed
        for idx in current_group_indices:
            processed_indices.add(idx)

        # Consolidate data for the current group
        if current_group_indices:
            # Select all rows belonging to this group
            group_df = df.loc[list(current_group_indices)]

            consolidated_row = {}

            # --- Handle 'subject' column with 1.1, 1.2 numbering ---
            # Get unique subjects from the current group for numbering
            unique_group_subjects = group_df['subject'].astype(str).unique()
            consolidated_subject_str, subject_to_num_map = format_subject_output(unique_group_subjects)
            consolidated_row['subject'] = consolidated_subject_str

            # --- Handle 'sub_disposition_name' with (1.X, 1.Y) format ---
            if 'sub_disposition_name' in df.columns:
                consolidated_row['sub_disposition_name'] = format_sub_disposition_output(group_df, subject_to_num_map)
            else:
                consolidated_row['sub_disposition_name'] = "" # Or handle as generic list if column exists but no specific format required.

            # --- Handle 'docket_no' and 'DecodedBody' with simple 1. 2. numbering ---
            if 'docket_no' in df.columns:
                # Extract all individual docket numbers (handle semicolon separation)
                all_dockets = []
                for docket_cell in group_df['docket_no'].astype(str):
                    if docket_cell.lower() != 'nan':
                        all_dockets.extend([d.strip() for d in docket_cell.split(';') if d.strip()])
                unique_dockets = sorted(list(set(all_dockets)))
                consolidated_row['docket_no'] = format_simple_numbered_list(unique_dockets)
            else:
                consolidated_row['docket_no'] = ""

            if 'DecodedBody' in df.columns:
                unique_decoded_bodies = group_df['DecodedBody'].astype(str).unique()
                consolidated_row['DecodedBody'] = format_simple_numbered_list(unique_decoded_bodies)
            else:
                consolidated_row['DecodedBody'] = ""


            # --- Handle all other columns with simple 1. 2. numbering ---
            for col in df.columns:
                if col in ['subject', 'sub_disposition_name', 'docket_no', 'DecodedBody']:
                    continue # Already handled

                # Get unique values for the column within the group
                unique_values = group_df[col].astype(str).unique()
                # Use the simple numbered list format
                consolidated_row[col] = format_simple_numbered_list(unique_values)

            consolidated_rows.append(consolidated_row)

    # Create a new DataFrame from consolidated rows, ensuring all original columns are present
    # This creates an empty dataframe with all columns from the original df
    all_original_columns = df.columns.tolist()
    consolidated_df = pd.DataFrame(consolidated_rows, columns=all_original_columns)
    
    # Fill any NaN values in the consolidated DataFrame with empty strings for cleaner output
    consolidated_df = consolidated_df.fillna('')

    return consolidated_df

# --- Main execution ---
# Define the path to the uploaded CSV file
file_path = 'ticket_details_2024_01.csv'
output_file_path = 'consolidated_tickets_formatted.csv' # Changed output file name for clarity

# Set your desired similarity threshold (e.g., 75, 80, 85, etc.)
# A higher threshold means stricter similarity.
SIMILARITY_THRESHOLD = 80

print(f"Consolidating tickets from '{file_path}' with a similarity threshold of {SIMILARITY_THRESHOLD}...")
consolidated_df = consolidate_tickets(file_path, SIMILARITY_THRESHOLD)

if not consolidated_df.empty:
    try:
        consolidated_df.to_csv(output_file_path, index=False)
        print(f"Consolidation complete! Consolidated data saved to '{output_file_path}'")
        print(f"\nTotal original rows: {len(pd.read_csv(file_path))}")
        print(f"Total consolidated rows: {len(consolidated_df)}")
        print("\n--- First 5 rows of Consolidated Data (Partial View) ---")
        # Print a selection of columns for preview as DecodedBody can be very long
        preview_columns = [col for col in consolidated_df.columns if col not in ['DecodedBody']]
        if 'subject' in consolidated_df.columns:
            print(consolidated_df[['subject', 'sub_disposition_name', 'docket_no']].head().to_markdown(index=False))
        else:
             print(consolidated_df.head().to_markdown(index=False))
        print("\n(Note: Full DecodedBody content and other columns are in the CSV file)")

    except Exception as e:
        print(f"Error saving consolidated data to CSV: {e}", file=sys.stderr)
else:
    print("No data to consolidate or an error occurred during processing.")

