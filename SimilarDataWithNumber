import pandas as pd
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
import numpy as np
import sys
from collections import defaultdict

# --- Helper Formatting Functions ---

def format_simple_numbered_list(values):
    """
    Formats a list of values into a numbered string (e.g., "1. ValueA\n2. ValueB").
    Handles cases where 'values' might be a numpy array.
    Each item will be on a new line.
    """
    if len(values) == 0:
        return ""
    formatted_items = []
    for i, val in enumerate(values):
        # Ensure value is treated as string to avoid errors with non-string types.
        formatted_items.append(f"{i + 1}. {str(val).strip()}") # .strip() to clean whitespace
    return "\n".join(formatted_items)

def create_docket_sequence_map(unique_dockets_in_group):
    """
    Creates a mapping from actual docket numbers to their sequence numbers (1, 2, 3...)
    within a consolidated group, and returns the formatted docket string.
    """
    docket_to_seq_num_map = {}
    formatted_dockets = []
    sorted_dockets = sorted(list(unique_dockets_in_group)) # Ensure consistent order

    for i, docket in enumerate(sorted_dockets):
        seq_num = i + 1
        docket_to_seq_num_map[docket] = str(seq_num)
        formatted_dockets.append(f"{seq_num}. {docket}")
    return "\n".join(formatted_dockets), docket_to_seq_num_map

def format_docket_specific_content(group_df, col_name, docket_to_seq_num_map):
    """
    Merges content (DecodedBody, Subject, Problem Reported) based on individual docket numbers,
    prefixing with the docket's sequence number.
    Format: "1. [Content for Docket A]\n2. [Content for Docket B]"
    """
    docket_content_map = defaultdict(list)
    
    # Iterate through each row in the group DataFrame (rows from the similar-subject group)
    for index, row in group_df.iterrows():
        # Get individual dockets from this row's 'docket_no' cell
        row_dockets_str = str(row['docket_no'])
        row_dockets = [d.strip() for d in row_dockets_str.split(';') if d.strip()]
        
        content = str(row[col_name]).strip()
        
        # Associate content with each individual docket in this row
        for docket in row_dockets:
            if docket in docket_to_seq_num_map: # Only if this docket is part of the current consolidated group's dockets
                docket_content_map[docket].append(content)
    
    formatted_output = []
    # Sort by docket sequence number for consistent output
    sorted_dockets_in_group = sorted(docket_to_seq_num_map.keys(), key=lambda d: int(docket_to_seq_num_map[d]))

    for docket in sorted_dockets_in_group:
        seq_num = docket_to_seq_num_map[docket]
        # Get unique content for this specific docket and join them with spaces
        unique_content_for_docket = " ".join(sorted(list(set(docket_content_map[docket]))))
        
        if unique_content_for_docket:
            formatted_output.append(f"{seq_num}. {unique_content_for_docket}")
    
    return "\n".join(formatted_output)

def format_docket_attribute_grouping(group_df, attribute_col_name, docket_to_seq_num_map):
    """
    Formats Priority, Disposition, Sub-disposition with associated docket sequence numbers.
    Format: "(1,2,3) Semicritical\n(4,5,6) Non Critical"
    """
    attribute_to_docket_seq_nums = defaultdict(set) # Maps attribute value to a set of docket sequence numbers

    for index, row in group_df.iterrows():
        # Get individual dockets from this row's 'docket_no' cell
        row_dockets_str = str(row['docket_no'])
        row_dockets = [d.strip() for d in row_dockets_str.split(';') if d.strip()]
        
        attribute_value = str(row[attribute_col_name]).strip()

        # If attribute value is not 'nan' or empty, associate dockets with it
        if attribute_value and attribute_value.lower() != 'nan':
            for docket in row_dockets:
                if docket in docket_to_seq_num_map: # Ensure docket is part of the current consolidated group's dockets
                    attribute_to_docket_seq_nums[attribute_value].add(docket_to_seq_num_map[docket])

    formatted_attributes = []
    # Sort attribute values for consistent output order
    sorted_attribute_values = sorted(attribute_to_docket_seq_nums.keys())

    for attr_val in sorted_attribute_values:
        # Sort docket sequence numbers numerically
        docket_seq_nums_sorted = sorted(list(attribute_to_docket_seq_nums[attr_val]), key=int)
        docket_nums_str = ", ".join(docket_seq_nums_sorted)
        formatted_attributes.append(f"({docket_nums_str}){attr_val}")

    return "\n".join(formatted_attributes)


def consolidate_tickets(file_path, similarity_threshold=80):
    """
    Scans a CSV file, identifies rows with similar subjects (primary grouping),
    and then consolidates other columns based on docket numbers within those groups.

    Args:
        file_path (str): The path to the input CSV file.
        similarity_threshold (int): The minimum fuzzy matching score (0-100)
                                    to consider two subjects similar.

    Returns:
        pd.DataFrame: A new DataFrame with consolidated rows.
    """
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found. Please ensure the file exists.", file=sys.stderr)
        return pd.DataFrame()
    except Exception as e:
        print(f"Error reading CSV file: {e}", file=sys.stderr)
        return pd.DataFrame()

    # Ensure required columns exist, add if missing for robustness
    required_cols = ['docket_no', 'subject', 'problem_reported', 'priority_name', 
                     'disposition_name', 'sub_disposition_name', 'DecodedBody']
    for col in required_cols:
        if col not in df.columns:
            print(f"Warning: Column '{col}' not found. Adding empty column.", file=sys.stderr)
            df[col] = '' 
    
    # Fill any NaN values in the input DataFrame with empty strings before processing
    df = df.fillna('')

    consolidated_rows_output = []
    processed_indices = set()

    # Get a list of subjects and their original indices for primary grouping
    subjects_with_indices = [(str(sub), i) for i, sub in enumerate(df['subject'])]

    for i, (current_subject, current_idx) in enumerate(subjects_with_indices):
        if current_idx in processed_indices:
            continue # Skip if this row has already been processed as part of a group

        # Step 1: Identify group based on similar subject
        current_group_indices = {current_idx}
        
        for j, (other_subject, other_idx) in enumerate(subjects_with_indices):
            if other_idx in processed_indices or other_idx == current_idx:
                continue

            # Calculate similarity using token_sort_ratio for better robustness to word order
            score = fuzz.token_sort_ratio(current_subject, other_subject)

            if score >= similarity_threshold:
                current_group_indices.add(other_idx)
        
        # Mark all rows in this subject-similar group as processed
        for idx in current_group_indices:
            processed_indices.add(idx)

        # Process the current subject-based group
        if current_group_indices:
            group_df = df.loc[list(current_group_indices)].copy()

            consolidated_row_data = {}
            
            # --- Generate Docket Numbering and Map for this specific group ---
            all_dockets_in_group = []
            for docket_cell in group_df['docket_no'].astype(str):
                # Split and extend to get individual dockets
                all_dockets_in_group.extend([d.strip() for d in docket_cell.split(';') if d.strip()])
            unique_dockets_in_group = set(all_dockets_in_group) # Get unique individual dockets
            
            # Create the 1., 2. numbering for dockets and the mapping
            formatted_dockets_str, docket_to_seq_num_map = create_docket_sequence_map(unique_dockets_in_group)
            consolidated_row_data['docket_no'] = formatted_dockets_str

            # --- Consolidate other columns based on docket numbers ---
            consolidated_row_data['DecodedBody'] = format_docket_specific_content(group_df, 'DecodedBody', docket_to_seq_num_map)
            consolidated_row_data['subject'] = format_docket_specific_content(group_df, 'subject', docket_to_seq_num_map)
            consolidated_row_data['problem_reported'] = format_docket_specific_content(group_df, 'problem_reported', docket_to_seq_num_map)

            # --- Consolidate attributes with (seq,seq)Value format ---
            consolidated_row_data['priority_name'] = format_docket_attribute_grouping(group_df, 'priority_name', docket_to_seq_num_map)
            consolidated_row_data['disposition_name'] = format_docket_attribute_grouping(group_df, 'disposition_name', docket_to_seq_num_map)
            consolidated_row_data['sub_disposition_name'] = format_docket_attribute_grouping(group_df, 'sub_disposition_name', docket_to_seq_num_map)

            # --- Handle remaining columns with simple 1. 2. numbering ---
            # Define all columns from the original DataFrame
            all_original_columns = df.columns.tolist()
            for col in all_original_columns:
                # Skip columns already processed above
                if col not in consolidated_row_data:
                    unique_values = group_df[col].astype(str).unique()
                    consolidated_row_data[col] = format_simple_numbered_list(unique_values)
            
            # Add the fully formed consolidated row to the output list
            # Ensure all original columns are present in the final row, filling with empty if not consolidated
            final_row_dict = {col: consolidated_row_data.get(col, '') for col in all_original_columns}
            consolidated_rows_output.append(final_row_dict)

    # Create a new DataFrame from the consolidated rows
    all_original_columns = df.columns.tolist() # Re-get columns in case any were added
    consolidated_df = pd.DataFrame(consolidated_rows_output, columns=all_original_columns)
    
    # Final fillna for any cells that might still be NaN (should be rare with current logic)
    consolidated_df = consolidated_df.fillna('')

    return consolidated_df

# --- Main execution ---
# Define the path to the uploaded CSV file
file_path = 'ticket_details_2024_01.csv'
output_file_path = 'Demo2consolidated_tickets_formatted.csv' 

# Set your desired similarity threshold (e.g., 75, 80, 85, etc.)
# A higher threshold means stricter similarity.
SIMILARITY_THRESHOLD = 80

print(f"Consolidating tickets from '{file_path}' based on similar subjects (threshold={SIMILARITY_THRESHOLD})...")
consolidated_df = consolidate_tickets(file_path, SIMILARITY_THRESHOLD)

if not consolidated_df.empty:
    try:
        consolidated_df.to_csv(output_file_path, index=False)
        print(f"Consolidation complete! Consolidated data saved to '{output_file_path}'")
        print(f"\nTotal original rows: {len(pd.read_csv(file_path))}")
        print(f"Total consolidated rows: {len(consolidated_df)}")
        print("\n--- First 5 rows of Consolidated Data (Preview of Key Columns) ---")
        
        # Define key columns for preview
        preview_cols = [
            'docket_no', 
            'subject', 
            'problem_reported', 
            'priority_name', 
            'disposition_name', 
            'sub_disposition_name'
        ]
        
        # Filter to only show columns that actually exist in the consolidated DataFrame
        existing_preview_cols = [col for col in preview_cols if col in consolidated_df.columns]
        
        if existing_preview_cols:
            print(consolidated_df[existing_preview_cols].head().to_markdown(index=False))
        else:
             print(consolidated_df.head().to_markdown(index=False)) # Fallback if none of the key columns exist
        print("\n(Note: Full content for all columns, including DecodedBody, is in the CSV file. Enable 'Wrap Text' in spreadsheet for full view.)")

    except Exception as e:
        print(f"Error saving consolidated data to CSV: {e}", file=sys.stderr)
else:
    print("No data to consolidate or an error occurred during processing.")
