# -*- coding: utf-8 -*-
"""clustering_usingLargeModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q_hZePN7riLfE72I1PLJIPP4HRUHUT1m
"""

import nltk
nltk.download('punkt')

import pandas as pd
from sentence_transformers import SentenceTransformer
import hdbscan
from collections import defaultdict

# Step 1: Load the Excel file and problem_reported column
df = pd.read_excel("full_cleand_data_for_rag.xlsx")  # Reading Excel file

column = 'problem_reported'

# Step 2: Extract cleaned problem_reported text
if column not in df.columns:
    raise ValueError("Column 'problem_reported' not found in the Excel file.")
texts = df[column].fillna('').tolist()  # Replace NaN with empty string

# Step 3: Convert problem_reported texts to sentence embeddings
model = SentenceTransformer('sentence-t5-large')
embeddings = model.encode(texts, show_progress_bar=True)

# Step 4: Run HDBSCAN clustering
clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric='euclidean')
labels = clusterer.fit_predict(embeddings)

# Step 5: Save cluster labels back to the DataFrame
df['Cluster'] = labels

# Step 6: Save the updated DataFrame to a new CSV file
df.to_csv("clustered_emails.csv", index=False)  # Saving as CSV
print("Clustered tickets saved to 'clustered_emails.csv'.")

# (Optional) Step 7: Print clusters to console
clusters = defaultdict(list)
for i, label in enumerate(labels):
    clusters[label].append(texts[i])

for label, grouped_texts in clusters.items():
    print(f"\nðŸ§  Intent Cluster {label} ({len(grouped_texts)} tickets):")
    for text in grouped_texts[:5]:  # Print only first 5 per cluster to avoid overload
        print(f" - {text}")

